{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI and SERP API keys\n",
    "\n",
    "openai_api_key = os.getenv('openai_api_key')\n",
    "serp_api_key = os.getenv('serp_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with API key\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Capture INFO level and above\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Simple format with timestamp, log level, and message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scraping function to extract content for a given URL\n",
    "def scrape_targeted_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract basic content\n",
    "        #title = soup.title.string if soup.title else ''\n",
    "        #meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "        #meta_description = meta_description['content'] if meta_description else ''\n",
    "        body_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "        \n",
    "        return {\n",
    "            #\"title\": title,\n",
    "            #\"meta_description\": meta_description,\n",
    "            \"body_text\": body_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {url}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search function using SERP API\n",
    "\n",
    "def search_company(company_name):\n",
    "    search_url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": company_name,\n",
    "        \"api_key\": serp_api_key  # Use the SERP API key here\n",
    "    }\n",
    "    response = requests.get(search_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()\n",
    "        results = []\n",
    "        for result in search_results.get('organic_results', [])[:3]:  # Limit to top 3 results\n",
    "            results.append({\n",
    "                'url': result.get('link'),\n",
    "                'text': result.get('snippet', '')\n",
    "            })\n",
    "        return results\n",
    "    else:\n",
    "        logging.error(f\"Request failed with status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to summarize search results' content extracted from scraping using Open AI\n",
    "\n",
    "def summarize_search_results(search_results):\n",
    "    all_texts = []\n",
    "    for result in search_results:\n",
    "        content = scrape_targeted_content(result['url'])\n",
    "        text = content.get(\"body_text\", \"\")\n",
    "        if text:\n",
    "            all_texts.append(text)\n",
    "        time.sleep(1)  # Respectful delay between requests\n",
    "    \n",
    "    combined_text = ' '.join(all_texts)\n",
    "    max_length = 1000  # Maximum token limit for GPT-4 is around 4096 tokens\n",
    "    if len(combined_text) > max_length:\n",
    "        combined_text = combined_text[:max_length]\n",
    "    \n",
    "    print(f\"Combined text for summarization:\\n{combined_text[:1000]}...\")  # Print only the first 500 characters for brevity\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following text: {combined_text}\"}\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    #summary = response['choices'][0]['message']['content'].strip()\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the functions to create an AI sales agent\n",
    "\n",
    "def ai_sales_agent(company_name):\n",
    "    search_results = search_company(company_name)\n",
    "    if search_results:\n",
    "        for result in search_results:\n",
    "            content = scrape_targeted_content(result['url'])\n",
    "            #print(f\"URL: {result['url']}\\nTitle: {content['title']}\\nMeta Description: {content['meta_description']}\\nBody Text: {content['body_text'][:200]}...\")  # Print only the first 200 characters of the body text\n",
    "        \n",
    "        summary = summarize_search_results(search_results)\n",
    "        results_output = \"\\n\\n\".join([f\"URL: {result['url']}\\nText: {result['text']}\" for result in search_results])\n",
    "        return f\"Summary for {company_name}:\\n{summary}\\n\\nURLs identified:\\n{results_output}\"\n",
    "    else:\n",
    "        return \"No results found for the company name.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the AI sales agent for a given company name\n",
    "\n",
    "company_name = \"Shell\"  # Replace with the company name you want to search for\n",
    "summary = ai_sales_agent(company_name)\n",
    "print(f\"Summary for {company_name}:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
