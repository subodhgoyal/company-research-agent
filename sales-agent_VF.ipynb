{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: openai in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (1.35.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrutu\\desktop\\account research agent\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import logging\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI and SERP API keys\n",
    "\n",
    "openai_api_key = 'sk-proj-lXv4D7Ep2mVnjng7i58sT3BlbkFJDm0BMQKrCn2mv56VLs1S'\n",
    "serp_api_key = 'c5447badf6cccf8421758fa2969175d1c32f08368b47fa4a62cd340d6525d40e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with API key\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Capture INFO level and above\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Simple format with timestamp, log level, and message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scraping function to extract content for a given URL\n",
    "def scrape_targeted_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract basic content\n",
    "        #title = soup.title.string if soup.title else ''\n",
    "        #meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "        #meta_description = meta_description['content'] if meta_description else ''\n",
    "        body_text = ' '.join(p.get_text() for p in soup.find_all('p'))\n",
    "        \n",
    "        return {\n",
    "            #\"title\": title,\n",
    "            #\"meta_description\": meta_description,\n",
    "            \"body_text\": body_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {url}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search function using SERP API\n",
    "\n",
    "def search_company(company_name):\n",
    "    search_url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": company_name,\n",
    "        \"api_key\": serp_api_key  # Use the SERP API key here\n",
    "    }\n",
    "    response = requests.get(search_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()\n",
    "        results = []\n",
    "        for result in search_results.get('organic_results', [])[:3]:  # Limit to top 3 results\n",
    "            results.append({\n",
    "                'url': result.get('link'),\n",
    "                'text': result.get('snippet', '')\n",
    "            })\n",
    "        return results\n",
    "    else:\n",
    "        logging.error(f\"Request failed with status code: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to summarize search results' content extracted from scraping using Open AI\n",
    "\n",
    "def summarize_search_results(search_results):\n",
    "    all_texts = []\n",
    "    for result in search_results:\n",
    "        content = scrape_targeted_content(result['url'])\n",
    "        text = content.get(\"body_text\", \"\")\n",
    "        if text:\n",
    "            all_texts.append(text)\n",
    "        time.sleep(1)  # Respectful delay between requests\n",
    "    \n",
    "    combined_text = ' '.join(all_texts)\n",
    "    max_length = 1000  # Maximum token limit for GPT-4 is around 4096 tokens\n",
    "    if len(combined_text) > max_length:\n",
    "        combined_text = combined_text[:max_length]\n",
    "    \n",
    "    print(f\"Combined text for summarization:\\n{combined_text[:1000]}...\")  # Print only the first 500 characters for brevity\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Summarize the following text: {combined_text}\"}\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "        max_tokens=300,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    #summary = response['choices'][0]['message']['content'].strip()\n",
    "    summary = response.choices[0].message.content\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the functions to create an AI sales agent\n",
    "\n",
    "def ai_sales_agent(company_name):\n",
    "    search_results = search_company(company_name)\n",
    "    if search_results:\n",
    "        for result in search_results:\n",
    "            content = scrape_targeted_content(result['url'])\n",
    "            #print(f\"URL: {result['url']}\\nTitle: {content['title']}\\nMeta Description: {content['meta_description']}\\nBody Text: {content['body_text'][:200]}...\")  # Print only the first 200 characters of the body text\n",
    "        \n",
    "        summary = summarize_search_results(search_results)\n",
    "        results_output = \"\\n\\n\".join([f\"URL: {result['url']}\\nText: {result['text']}\" for result in search_results])\n",
    "        return f\"Summary for {company_name}:\\n{summary}\\n\\nURLs identified:\\n{results_output}\"\n",
    "    else:\n",
    "        return \"No results found for the company name.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text for summarization:\n",
      "We’re finding ways to bring energy to more people in more ways every day, so that all of us can be a part of the changing energy system. Because Powering Progress means providing more and cleaner energy across the country.   Read more Read more Read more Read more See how! Read the full article Read the full article Read the full article Read the full article Read the full article Watch the video! Read the full article Read the full article Read the full article Read the full article Read the full article Read past articles, press releases, and announcements.    Gretchen Watkins, President, Shell USA, Inc.  We’re finding ways to bring energy to more people in more ways every day, so that all of us can be a part of the changing energy system. Because Powering Progress means providing more and cleaner energy across the country.   Get the most out of driving. Discover the wide range of products Shell offers to improve your driving experience, and programs to save money when doing so. Make...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 23:03:50,307 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for Shell:\n",
      "Summary for Shell:\n",
      "The text discusses efforts to distribute energy to more people in diverse ways daily, thus allowing everyone to participate in the evolving energy system. Powering Progress aims to provide increased and cleaner energy nationwide. Shell USA's President, Gretchen Watkins, also emphasizes this theme. Furthermore, Shell offers a variety of products and programs to enhance the driving experience and save money while doing so.\n",
      "\n",
      "URLs identified:\n",
      "URL: https://www.shell.com/\n",
      "Text: Shell's award-winning digital stories channel. Our team of writers and reporters offer fresh insights into energy, technology and the people and ideas powering ...\n",
      "\n",
      "URL: https://www.shell.us/\n",
      "Text: Shell in the United States explores and produces energy products - fuels, oil, natural gas, lubricants, LPG, chemicals; with major projects in the Gulf of ...\n",
      "\n",
      "URL: https://en.wikipedia.org/wiki/Shell_plc\n",
      "Text: Shell plc is a British multinational oil and gas company headquartered in London, England. Shell is a public limited company with a primary listing on the ...\n"
     ]
    }
   ],
   "source": [
    "# Run the AI sales agent for a given company name\n",
    "\n",
    "company_name = \"Shell\"  # Replace with the company name you want to search for\n",
    "summary = ai_sales_agent(company_name)\n",
    "print(f\"Summary for {company_name}:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
